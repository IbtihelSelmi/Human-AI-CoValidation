# Human‚ÄìAI Co-Validation for Clinical LLM Outputs

A lightweight prototype that shows how human verification and AI cross-checking can work together to reduce hallucinations in clinical text generated by Large Language Models (LLMs).

---

## ‚úÖ What the app does
- Takes clinical text as input
- Applies structured verification rules
- Flags possible hallucinations or safety risks
- Returns a feedback message instead of unverified outputs

---

## üß† Why it matters
LLMs are useful in healthcare but can generate convincing yet incorrect information. This prototype explores a practical validation loop combining:
- AI pre-analysis
- Rule-based checking
- Human oversight logic

---

## üõ† Tech stack
- Python 3.x
- Flask (UI + API)
- Simple rule engine (prototype level)

---

## ‚ñ∂Ô∏è Run the project locally

1. Clone the repository:

